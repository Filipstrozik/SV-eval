{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import os\n",
    "import wespeaker\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import subprocess\n",
    "import sys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_dir = \"./embeds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_directory_voxceleb1(test_dir):\n",
    "    data = []\n",
    "    for person_id in os.listdir(test_dir):\n",
    "        person_path = os.path.join(test_dir, person_id)\n",
    "        if os.path.isdir(person_path):\n",
    "            for utterance_env in os.listdir(person_path):\n",
    "                utterance_path = os.path.join(person_path, utterance_env)\n",
    "                if os.path.isdir(utterance_path):\n",
    "                    for file in os.listdir(utterance_path):\n",
    "                        file_path = os.path.join(utterance_path, file)\n",
    "                        if os.path.isfile(file_path):\n",
    "                            # Assuming embedding is a placeholder for actual embedding extraction\n",
    "                            data.append([file_path, person_id, utterance_env, file])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=['path', 'person_id', 'utterance_env', 'utterance_filename'])\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "test_dir = '../data/vox1_test_wav'\n",
    "df = scan_directory_voxceleb1(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_directory_voxceleb2(test_dir):\n",
    "    data = []\n",
    "    for person_id in os.listdir(test_dir):\n",
    "        person_path = os.path.join(test_dir, person_id)\n",
    "        if os.path.isdir(person_path):\n",
    "            for file in os.listdir(person_path):\n",
    "                file_path = os.path.join(person_path, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Assuming embedding is a placeholder for actual embedding extraction\n",
    "                    data.append([file_path, person_id, file])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['path', 'person_id', 'utterance_filename'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "test_dir = '../data/voxceleb2_eval_segments'\n",
    "df = scan_directory_voxceleb2(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>person_id</th>\n",
       "      <th>utterance_filename</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id02019/50_seg...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>50_seg3_.wav</td>\n",
       "      <td>embedding_placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id02019/194_se...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>194_seg0_.wav</td>\n",
       "      <td>embedding_placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id02019/196_se...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>196_seg8_.wav</td>\n",
       "      <td>embedding_placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id02019/87_seg...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>87_seg8_.wav</td>\n",
       "      <td>embedding_placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id02019/1_seg9...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>1_seg9_.wav</td>\n",
       "      <td>embedding_placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id03347/268_se...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>268_seg8_.wav</td>\n",
       "      <td>embedding_placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11092</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id03347/323_se...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>323_seg4_.wav</td>\n",
       "      <td>embedding_placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11093</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id03347/313_se...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>313_seg17_.wav</td>\n",
       "      <td>embedding_placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11094</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id03347/259_se...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>259_seg15_.wav</td>\n",
       "      <td>embedding_placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11095</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id03347/160_se...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>160_seg0_.wav</td>\n",
       "      <td>embedding_placeholder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11096 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path person_id  \\\n",
       "0      ../data/voxceleb2_eval_segments/id02019/50_seg...   id02019   \n",
       "1      ../data/voxceleb2_eval_segments/id02019/194_se...   id02019   \n",
       "2      ../data/voxceleb2_eval_segments/id02019/196_se...   id02019   \n",
       "3      ../data/voxceleb2_eval_segments/id02019/87_seg...   id02019   \n",
       "4      ../data/voxceleb2_eval_segments/id02019/1_seg9...   id02019   \n",
       "...                                                  ...       ...   \n",
       "11091  ../data/voxceleb2_eval_segments/id03347/268_se...   id03347   \n",
       "11092  ../data/voxceleb2_eval_segments/id03347/323_se...   id03347   \n",
       "11093  ../data/voxceleb2_eval_segments/id03347/313_se...   id03347   \n",
       "11094  ../data/voxceleb2_eval_segments/id03347/259_se...   id03347   \n",
       "11095  ../data/voxceleb2_eval_segments/id03347/160_se...   id03347   \n",
       "\n",
       "      utterance_filename              embedding  \n",
       "0           50_seg3_.wav  embedding_placeholder  \n",
       "1          194_seg0_.wav  embedding_placeholder  \n",
       "2          196_seg8_.wav  embedding_placeholder  \n",
       "3           87_seg8_.wav  embedding_placeholder  \n",
       "4            1_seg9_.wav  embedding_placeholder  \n",
       "...                  ...                    ...  \n",
       "11091      268_seg8_.wav  embedding_placeholder  \n",
       "11092      323_seg4_.wav  embedding_placeholder  \n",
       "11093     313_seg17_.wav  embedding_placeholder  \n",
       "11094     259_seg15_.wav  embedding_placeholder  \n",
       "11095      160_seg0_.wav  embedding_placeholder  \n",
       "\n",
       "[11096 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mp4_to_wav(mp4_path, wav_path):\n",
    "    command = [\"ffmpeg\", \"-i\", mp4_path, wav_path]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"../data/voxceleb2_wav_eval\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the dataframe and convert each MP4 file to WAV\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Converting MP4 to WAV\"):\n",
    "    # Create a directory for each person_id if it doesn't exist\n",
    "    person_dir = os.path.join(output_dir, row['person_id'])\n",
    "    os.makedirs(person_dir, exist_ok=True)\n",
    "    \n",
    "    mp4_file = row['path']\n",
    "    wav_file = os.path.join(person_dir, os.path.splitext(os.path.basename(mp4_file))[0] + \".wav\")\n",
    "    convert_mp4_to_wav(mp4_file, wav_file)\n",
    "    print(f\"Converted {mp4_file} to {wav_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_len):\n",
    "        self.dataframe = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_path = self.dataframe.iloc[idx]['path']\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        waveform = self.pad_or_cut_wave(waveform, max_len)\n",
    "\n",
    "        sample = {'path': audio_path, 'waveform': waveform, 'sample_rate': sample_rate}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def pad_or_cut_wave(self, data, max_len):\n",
    "        \"\"\"Pad or cut a single wave to the specified length.\n",
    "\n",
    "        Args:\n",
    "            data: torch.Tensor (random len)\n",
    "            max_len: maximum length to pad or cut the data\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor (padded or cut to max_len)\n",
    "        \"\"\"\n",
    "        data_len = data.shape[1]\n",
    "        if data_len < max_len:\n",
    "            padding = max_len - data_len\n",
    "            data = torch.nn.functional.pad(data, (0, padding))\n",
    "        else:\n",
    "            data = data[:, :max_len]\n",
    "        return data\n",
    "\n",
    "# Create an instance of the dataset\n",
    "# 1 s  = 16_000 samples\n",
    "max_len = 5 * 16000\n",
    "audio_dataset = AudioDataset(df, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader for the audio_dataset\n",
    "audio_dataloader = DataLoader(audio_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "campplus_model = wespeaker.load_model(\"campplus\")\n",
    "campplus_model.set_device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatasetFBank(Dataset):\n",
    "    def __init__(self, dataframe, max_len, model):\n",
    "        self.dataframe = dataframe\n",
    "        self.max_len = max_len\n",
    "        self.model = model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_path = self.dataframe.iloc[idx][\"path\"]\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        waveform = self.pad_or_cut_wave(waveform, max_len)\n",
    "\n",
    "        # Extract fbank features\n",
    "        fbank = self.model.compute_fbank(waveform)\n",
    "\n",
    "        sample = {\"path\": audio_path, \"fbank\": fbank, \"sample_rate\": sample_rate}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def pad_or_cut_wave(self, data, max_len):\n",
    "        \"\"\"Pad or cut a single wave to the specified length.\n",
    "\n",
    "        Args:\n",
    "            data: torch.Tensor (random len)\n",
    "            max_len: maximum length to pad or cut the data\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor (padded or cut to max_len)\n",
    "        \"\"\"\n",
    "        data_len = data.shape[1]\n",
    "        if data_len < max_len:\n",
    "            padding = max_len - data_len\n",
    "            data = torch.nn.functional.pad(data, (0, padding))\n",
    "        else:\n",
    "            data = data[:, :max_len]\n",
    "        return data\n",
    "    \n",
    "\n",
    "# Create an instance of the dataset\n",
    "# 1 s  = 16_000 samples\n",
    "max_len = 5 * 16000\n",
    "audio_dataset_fbank = AudioDatasetFBank(df, max_len, campplus_model)\n",
    "\n",
    "# Create a DataLoader for the audio_dataset\n",
    "audio_dataloader_fbank = DataLoader(audio_dataset_fbank, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "campplus_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:27<00:00,  5.59it/s]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_CAMPPLUS(we_speaker_model, dataloader):\n",
    "    all_embeddings = {}\n",
    "    we_speaker_model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            utts = batch[\"path\"]\n",
    "            features = batch[\"fbank\"].float().to(we_speaker_model.device)\n",
    "            # Forward through model\n",
    "            outputs = we_speaker_model.model(features)  # embed or (embed_a, embed_b)\n",
    "            embeds = outputs[-1] if isinstance(outputs, tuple) else outputs\n",
    "            embeds = embeds.cpu().detach().numpy()\n",
    "\n",
    "            for i, utt in enumerate(utts):\n",
    "                embed = embeds[i]\n",
    "                all_embeddings[utt] = embed\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "embeddings = evaluate_CAMPPLUS(campplus_model, audio_dataloader_fbank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embeddings_to_df(df, embeddings):\n",
    "    embeddings_df = pd.DataFrame(\n",
    "        list(embeddings.items()), columns=[\"path\", \"embedding\"]\n",
    "    )\n",
    "    df_with_embeddings = df.merge(embeddings_df, on=\"path\")\n",
    "    return df_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4874 entries, 0 to 4873\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   path                4874 non-null   object\n",
      " 1   person_id           4874 non-null   object\n",
      " 2   utterance_env       4874 non-null   object\n",
      " 3   utterance_filename  4874 non-null   object\n",
      " 4   embedding           4874 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 190.5+ KB\n"
     ]
    }
   ],
   "source": [
    "merged_df = map_embeddings_to_df(df, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>person_id</th>\n",
       "      <th>utterance_env</th>\n",
       "      <th>utterance_filename</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...</td>\n",
       "      <td>id10295</td>\n",
       "      <td>nt7dNRvlEHE</td>\n",
       "      <td>00005.wav</td>\n",
       "      <td>[-1.9477693, 0.87528807, 1.1467675, 1.1967306,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...</td>\n",
       "      <td>id10295</td>\n",
       "      <td>nt7dNRvlEHE</td>\n",
       "      <td>00004.wav</td>\n",
       "      <td>[-1.3404751, 1.564712, 1.1248378, 0.6840156, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...</td>\n",
       "      <td>id10295</td>\n",
       "      <td>nt7dNRvlEHE</td>\n",
       "      <td>00001.wav</td>\n",
       "      <td>[-1.6733762, 1.2464287, 0.46165344, 0.7650559,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...</td>\n",
       "      <td>id10295</td>\n",
       "      <td>nt7dNRvlEHE</td>\n",
       "      <td>00003.wav</td>\n",
       "      <td>[-1.5344026, 0.9411537, 1.0485039, -0.4902958,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...</td>\n",
       "      <td>id10295</td>\n",
       "      <td>nt7dNRvlEHE</td>\n",
       "      <td>00002.wav</td>\n",
       "      <td>[-0.84181863, 0.41680804, 0.8774067, 0.0415427...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path person_id utterance_env  \\\n",
       "0  ../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...   id10295   nt7dNRvlEHE   \n",
       "1  ../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...   id10295   nt7dNRvlEHE   \n",
       "2  ../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...   id10295   nt7dNRvlEHE   \n",
       "3  ../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...   id10295   nt7dNRvlEHE   \n",
       "4  ../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...   id10295   nt7dNRvlEHE   \n",
       "\n",
       "  utterance_filename                                          embedding  \n",
       "0          00005.wav  [-1.9477693, 0.87528807, 1.1467675, 1.1967306,...  \n",
       "1          00004.wav  [-1.3404751, 1.564712, 1.1248378, 0.6840156, -...  \n",
       "2          00001.wav  [-1.6733762, 1.2464287, 0.46165344, 0.7650559,...  \n",
       "3          00003.wav  [-1.5344026, 0.9411537, 1.0485039, -0.4902958,...  \n",
       "4          00002.wav  [-0.84181863, 0.41680804, 0.8774067, 0.0415427...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4874 entries, 0 to 4873\n",
      "Data columns (total 5 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   path                4874 non-null   object\n",
      " 1   person_id           4874 non-null   object\n",
      " 2   utterance_env       4874 non-null   object\n",
      " 3   utterance_filename  4874 non-null   object\n",
      " 4   embedding           4874 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 190.5+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_merged_df = merged_df.copy()\n",
    "# numpy_merged_df[\"embedding\"] = numpy_merged_df[\"embedding\"].apply(np.array)\n",
    "numpy_merged_df.to_parquet(\n",
    "    \"speaker_verification_data.parquet\",\n",
    "    index=False,\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded = pd.read_parquet(\"../embeds/vox1_test_wav/campplus_embeddings.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>person_id</th>\n",
       "      <th>utterance_env</th>\n",
       "      <th>utterance_filename</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...</td>\n",
       "      <td>id10295</td>\n",
       "      <td>nt7dNRvlEHE</td>\n",
       "      <td>00005.wav</td>\n",
       "      <td>[-0.66124964, 0.36657766, -1.1138742, 1.030842...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...</td>\n",
       "      <td>id10295</td>\n",
       "      <td>nt7dNRvlEHE</td>\n",
       "      <td>00004.wav</td>\n",
       "      <td>[-0.20502278, -0.88471156, -2.3427346, 0.82132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...</td>\n",
       "      <td>id10295</td>\n",
       "      <td>nt7dNRvlEHE</td>\n",
       "      <td>00001.wav</td>\n",
       "      <td>[-0.058600664, 0.6727131, -0.7362049, 0.832892...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...</td>\n",
       "      <td>id10295</td>\n",
       "      <td>nt7dNRvlEHE</td>\n",
       "      <td>00003.wav</td>\n",
       "      <td>[-0.14910118, 0.51563984, -0.8659944, 1.116852...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...</td>\n",
       "      <td>id10295</td>\n",
       "      <td>nt7dNRvlEHE</td>\n",
       "      <td>00002.wav</td>\n",
       "      <td>[-0.52327955, 0.759426, -0.7779301, 1.2519413,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path person_id utterance_env  \\\n",
       "0  ../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...   id10295   nt7dNRvlEHE   \n",
       "1  ../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...   id10295   nt7dNRvlEHE   \n",
       "2  ../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...   id10295   nt7dNRvlEHE   \n",
       "3  ../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...   id10295   nt7dNRvlEHE   \n",
       "4  ../data/vox1_test_wav/id10295/nt7dNRvlEHE/0000...   id10295   nt7dNRvlEHE   \n",
       "\n",
       "  utterance_filename                                          embedding  \n",
       "0          00005.wav  [-0.66124964, 0.36657766, -1.1138742, 1.030842...  \n",
       "1          00004.wav  [-0.20502278, -0.88471156, -2.3427346, 0.82132...  \n",
       "2          00001.wav  [-0.058600664, 0.6727131, -0.7362049, 0.832892...  \n",
       "3          00003.wav  [-0.14910118, 0.51563984, -0.8659944, 1.116852...  \n",
       "4          00002.wav  [-0.52327955, 0.759426, -0.7779301, 1.2519413,...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loaded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9477693 ,  0.87528807,  1.1467675 ,  1.1967306 , -0.47338516,\n",
       "        1.7572893 ,  0.35084888, -1.1540041 , -1.3210968 ,  1.6012275 ,\n",
       "       -1.0762585 , -0.44248694,  0.218608  , -0.27130088, -0.45462215,\n",
       "       -0.19865777, -0.26834065,  0.21367282, -0.8436834 ,  0.11194724,\n",
       "        0.01127281,  0.86356944, -0.22389542,  1.0766221 , -0.43149847,\n",
       "       -0.24521439, -1.0943152 ,  0.54799664, -2.165308  ,  0.42285034,\n",
       "        1.5470072 , -0.39065608, -0.21269657,  0.6641775 ,  1.0433041 ,\n",
       "       -0.6428348 , -0.24538065, -2.0366335 , -0.6788447 , -1.6342717 ,\n",
       "       -0.26556277,  0.03650101,  0.1763368 ,  0.8649897 , -0.80470115,\n",
       "       -1.825708  ,  1.4988852 , -1.1387967 ,  1.0510931 , -0.8013163 ,\n",
       "        1.6196156 , -1.1638414 ,  1.1226951 , -0.5127667 ,  0.02043838,\n",
       "       -1.0073698 , -1.9062557 ,  1.3125198 , -0.8931751 , -0.00470269,\n",
       "       -1.3825536 ,  0.02921643,  0.58605844, -2.123169  , -0.60786897,\n",
       "        0.7188853 ,  1.1662261 , -0.50129324, -0.14032638,  1.2586595 ,\n",
       "        0.14467931,  0.34490088, -1.1197215 ,  0.2684535 , -0.6803663 ,\n",
       "       -1.3187062 , -1.5144058 ,  0.66672647, -0.50402313,  0.02114888,\n",
       "       -0.4846425 ,  1.0565764 , -0.18358417, -0.8933918 ,  0.7964658 ,\n",
       "       -0.53905314,  0.16886637,  0.5587452 , -0.84897256,  2.064567  ,\n",
       "       -0.60090417,  1.3877606 , -0.98515457,  1.1539164 ,  0.09871989,\n",
       "       -2.6088884 ,  0.5395842 ,  0.7886598 , -0.75597787, -1.3672514 ,\n",
       "        0.97842175,  1.1457599 , -0.9546506 ,  0.9655058 , -0.09003496,\n",
       "       -0.554055  , -0.18267043,  1.0737325 ,  1.8114083 ,  0.05093873,\n",
       "        0.50104564,  0.5941351 , -2.559273  , -1.9476209 , -0.5327505 ,\n",
       "       -0.21525712, -0.07251845, -1.4647686 , -0.17509419,  0.64966726,\n",
       "        1.3079989 , -1.0304422 , -0.12588304, -0.30470997, -1.3090725 ,\n",
       "        0.22263335,  0.02923482, -1.277681  , -0.98104656,  0.16364828,\n",
       "       -1.6096617 , -0.4132379 , -1.4269278 ,  0.47463405,  0.07551495,\n",
       "        1.4548461 ,  1.4440792 , -2.124944  ,  0.97532886, -1.2204995 ,\n",
       "        1.2619259 ,  0.9274481 , -0.63897765, -0.594098  , -0.37088457,\n",
       "        1.0797151 , -0.06449372,  1.8843395 , -0.16473453, -0.5359702 ,\n",
       "       -0.48053303, -1.3834792 ,  1.6664076 ,  0.05027522,  0.7606984 ,\n",
       "        1.2919217 , -1.016452  , -0.7452123 ,  0.0842146 , -1.5870824 ,\n",
       "       -0.9578367 ,  0.643734  , -0.5751538 , -2.2111986 ,  1.5411459 ,\n",
       "        0.3840808 ,  0.16074921, -1.9745705 , -1.9114017 , -0.5780262 ,\n",
       "       -0.3825757 ,  0.33340678, -0.38450572, -0.7363666 , -0.70601696,\n",
       "       -0.8412628 , -0.95655566, -1.0600344 , -0.12154528, -0.65934616,\n",
       "       -0.21679647,  0.9224896 ,  1.1638299 , -1.138434  , -0.35680965,\n",
       "        0.30570984, -0.86764824, -0.3312944 , -0.20399559,  0.09616672,\n",
       "        0.37571076, -1.0051986 ], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get  embedding of the first row\n",
    "\n",
    "df_loaded.iloc[0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_loaded = np.stack(df_loaded[\"embedding\"].values)\n",
    "embeddings_tensor_loaded = torch.tensor(embeddings_loaded, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4874, 192])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_tensor_loaded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't infer object conversion type: 0       [-1.9477693, 0.87528807, 1.1467675, 1.1967306,...\n1       [-1.3404751, 1.564712, 1.1248378, 0.6840156, -...\n2       [-1.6733762, 1.2464287, 0.46165344, 0.7650559,...\n3       [-1.5344026, 0.9411537, 1.0485039, -0.4902958,...\n4       [-0.84181863, 0.41680804, 0.8774067, 0.0415427...\n                              ...                        \n4869    [0.104850784, 0.7097255, 1.407516, -0.96619225...\n4870    [-0.39940205, 0.86317617, 1.2550328, -0.467166...\n4871    [-0.15146518, 0.29355344, 1.1567106, -0.776961...\n4872    [-0.335996, 0.25943008, 1.2008984, -0.7599641,...\n4873    [0.073237926, 0.9998041, 0.6793622, -1.05732, ...\nName: embedding, Length: 4874, dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerged_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspeaker_verification_data.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msnappy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/studies/other/SV-eval/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/studies/other/SV-eval/.venv/lib/python3.12/site-packages/pandas/core/frame.py:3113\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   3032\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   3034\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3109\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   3110\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/studies/other/SV-eval/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:480\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    478\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 480\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/Documents/studies/other/SV-eval/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:349\u001b[0m, in \u001b[0;36mFastParquetImpl.write\u001b[0;34m(self, df, path, compression, index, partition_cols, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options passed with file object or non-fsspec file path\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/studies/other/SV-eval/.venv/lib/python3.12/site-packages/fastparquet/writer.py:1326\u001b[0m, in \u001b[0;36mwrite\u001b[0;34m(filename, data, row_group_offsets, compression, file_scheme, open_with, mkdirs, has_nulls, write_index, partition_on, fixed_text, append, object_encoding, times, custom_metadata, stats)\u001b[0m\n\u001b[1;32m   1323\u001b[0m check_column_names(data\u001b[38;5;241m.\u001b[39mcolumns, partition_on, fixed_text,\n\u001b[1;32m   1324\u001b[0m                    object_encoding, has_nulls)\n\u001b[1;32m   1325\u001b[0m ignore \u001b[38;5;241m=\u001b[39m partition_on \u001b[38;5;28;01mif\u001b[39;00m file_scheme \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m-> 1326\u001b[0m fmd \u001b[38;5;241m=\u001b[39m \u001b[43mmake_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_nulls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_nulls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfixed_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mobject_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_on\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcols_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_metadata:\n\u001b[1;32m   1332\u001b[0m     kvm \u001b[38;5;241m=\u001b[39m fmd\u001b[38;5;241m.\u001b[39mkey_value_metadata \u001b[38;5;129;01mor\u001b[39;00m []\n",
      "File \u001b[0;32m~/Documents/studies/other/SV-eval/.venv/lib/python3.12/site-packages/fastparquet/writer.py:922\u001b[0m, in \u001b[0;36mmake_metadata\u001b[0;34m(data, has_nulls, ignore_columns, fixed_text, object_encoding, times, index_cols, partition_cols, cols_dtype)\u001b[0m\n\u001b[1;32m    920\u001b[0m     se\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m column\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     se, \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mfind_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mobject_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mis_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m col_has_nulls \u001b[38;5;241m=\u001b[39m has_nulls\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_nulls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/studies/other/SV-eval/.venv/lib/python3.12/site-packages/fastparquet/writer.py:122\u001b[0m, in \u001b[0;36mfind_type\u001b[0;34m(data, fixed_text, object_encoding, times, is_index)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m object_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 122\u001b[0m         object_encoding \u001b[38;5;241m=\u001b[39m \u001b[43minfer_object_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m object_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28mtype\u001b[39m, converted_type, width \u001b[38;5;241m=\u001b[39m (parquet_thrift\u001b[38;5;241m.\u001b[39mType\u001b[38;5;241m.\u001b[39mBYTE_ARRAY,\n\u001b[1;32m    126\u001b[0m                                        parquet_thrift\u001b[38;5;241m.\u001b[39mConvertedType\u001b[38;5;241m.\u001b[39mUTF8,\n\u001b[1;32m    127\u001b[0m                                        \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/studies/other/SV-eval/.venv/lib/python3.12/site-packages/fastparquet/writer.py:375\u001b[0m, in \u001b[0;36minfer_object_encoding\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    373\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt infer object conversion type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m data)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't infer object conversion type: 0       [-1.9477693, 0.87528807, 1.1467675, 1.1967306,...\n1       [-1.3404751, 1.564712, 1.1248378, 0.6840156, -...\n2       [-1.6733762, 1.2464287, 0.46165344, 0.7650559,...\n3       [-1.5344026, 0.9411537, 1.0485039, -0.4902958,...\n4       [-0.84181863, 0.41680804, 0.8774067, 0.0415427...\n                              ...                        \n4869    [0.104850784, 0.7097255, 1.407516, -0.96619225...\n4870    [-0.39940205, 0.86317617, 1.2550328, -0.467166...\n4871    [-0.15146518, 0.29355344, 1.1567106, -0.776961...\n4872    [-0.335996, 0.25943008, 1.2008984, -0.7599641,...\n4873    [0.073237926, 0.9998041, 0.6793622, -1.05732, ...\nName: embedding, Length: 4874, dtype: object"
     ]
    }
   ],
   "source": [
    "merged_df.to_parquet(\n",
    "    \"speaker_verification_data.parquet\", index=False, compression=\"snappy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put embeddings into dataframe and save it to ./embeds/\n",
    "df['embedding'] = df['path'].map(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if embeddings match\n",
    "df['embedding'][0] == embeddings[df['path'][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings to disk\n",
    "\n",
    "os.makedirs(embeds_dir, exist_ok=True)\n",
    "csv_name = \"campplus_embeddings.csv\"\n",
    "df.to_csv(os.path.join(embeds_dir, csv_name), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECAPA_TDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecapa_model = wespeaker.load_model_local(\"./models/voxceleb_ECAPA1024\")\n",
    "ecapa_model.set_device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"./wav_files_voxceleb2\"\n",
    "ecapa_df = scan_directory_voxceleb2(test_dir)\n",
    "\n",
    "# Create an instance of the dataset\n",
    "# 1 s  = 16_000 samples\n",
    "max_len = 5 * 16000\n",
    "audio_dataset_fbank = AudioDatasetFBank(ecapa_df, max_len, ecapa_model)\n",
    "\n",
    "# Create a DataLoader for the audio_dataset\n",
    "audio_dataloader_fbank = DataLoader(audio_dataset_fbank, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ECAPA(we_speaker_model, dataloader):\n",
    "    all_embeddings = {}\n",
    "    we_speaker_model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            utts = batch[\"path\"]\n",
    "            features = batch[\"fbank\"].float().to(we_speaker_model.device)\n",
    "            # Forward through model\n",
    "            outputs = we_speaker_model.model(features)  # embed or (embed_a, embed_b)\n",
    "            embeds = outputs[-1] if isinstance(outputs, tuple) else outputs\n",
    "            embeds = embeds.cpu().detach().numpy()\n",
    "\n",
    "            for i, utt in enumerate(utts):\n",
    "                embed = embeds[i]\n",
    "                all_embeddings[utt] = embed\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "\n",
    "embeddings = evaluate_ECAPA(ecapa_model, audio_dataloader_fbank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecapa_df['embedding'] = ecapa_df['path'].map(embeddings)\n",
    "\n",
    "# check if embeddings match\n",
    "ecapa_df['embedding'][0] == embeddings[ecapa_df['path'][0]]\n",
    "\n",
    "# save embeddings to disk\n",
    "csv_name = \"ecapa_embeddings.csv\"\n",
    "ecapa_df.to_csv(os.path.join(embeds_dir, csv_name), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet34_model = wespeaker.load_model_local(\"./models/cnceleb_resnet34\")\n",
    "resnet34_model.set_device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 5 * 16000\n",
    "audio_dataset_fbank = AudioDatasetFBank(df, max_len, resnet34_model)\n",
    "audio_dataloader_fbank = DataLoader(audio_dataset_fbank, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_RESNET34(we_speaker_model, dataloader):\n",
    "    all_embeddings = {}\n",
    "    we_speaker_model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            utts = batch[\"path\"]\n",
    "            features = batch[\"fbank\"].float().to(we_speaker_model.device)\n",
    "            # Forward through model\n",
    "            outputs = we_speaker_model.model(features)  # embed or (embed_a, embed_b)\n",
    "            embeds = outputs[-1] if isinstance(outputs, tuple) else outputs\n",
    "            embeds = embeds.cpu().detach().numpy()\n",
    "\n",
    "            for i, utt in enumerate(utts):\n",
    "                embed = embeds[i]\n",
    "                all_embeddings[utt] = embed\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "\n",
    "embeddings = evaluate_RESNET34(resnet34_model, audio_dataloader_fbank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how many parameters in the model\n",
    "print(f\"Number of parameters in the model: {sum(p.numel() for p in resnet34_model.model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDIMNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Ustawienia\n",
    "repo_id = \"Jenthe/ECAPA2\"\n",
    "filename = \"ecapa2.pt\"\n",
    "cache_dir = \"../models/ReDimNet\"  # OkreÅ›l lokalizacjÄ™\n",
    "\n",
    "# Pobierz model\n",
    "model_file = hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "# model_name = \"b2\"  # ~b3-b4 size\n",
    "# train_type = \"ptn\"\n",
    "# dataset = \"vox2\"\n",
    "\n",
    "# redim = torch.hub.load(\n",
    "#     \"IDRnD/ReDimNet\",\n",
    "#     \"ReDimNet\",\n",
    "#     model_name=model_name,\n",
    "#     train_type=train_type,\n",
    "#     dataset=dataset,\n",
    "#     source=\"github\",\n",
    "# )\n",
    "\n",
    "# cache_dir = \"../models/ReDimNet\"\n",
    "# os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# torch.save(redim.state_dict(), os.path.join(cache_dir, \"redim_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../helper_libs\")\n",
    "from redimnet.model import ReDimNetWrap\n",
    "\n",
    "path = \"../models/ReDimNet/b6-vox2-ptn.pt\"\n",
    "full_state_dict = torch.load(path)\n",
    "model_config = full_state_dict[\"model_config\"]\n",
    "state_dict = full_state_dict[\"state_dict\"]\n",
    "\n",
    "# Create an instance of the model using the configuration\n",
    "redimnet_model = ReDimNetWrap(**model_config)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "redimnet_model.load_state_dict(state_dict)\n",
    "\n",
    "# Move the model to the desired device (e.g., 'mps' or 'cpu')\n",
    "redimnet_model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extract the model configuration and state dictionary from full_state_dict\n",
    "model_config = full_state_dict['model_config']\n",
    "state_dict = full_state_dict['state_dict']\n",
    "\n",
    "# Create an instance of the model using the configuration\n",
    "model = ReDimNetWrap(**model_config)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Move the model to the desired device (e.g., 'mps' or 'cpu')\n",
    "model.to('mps')\n",
    "\n",
    "# Verify the model is loaded correctly\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "path = \"../models/ReDimNet/redim_model.pt\"\n",
    "# Load the model from the local file\n",
    "model = torch.hub.load(\n",
    "\t\"IDRnD/ReDimNet\",\n",
    "\t\"ReDimNet\",\n",
    "\tmodel_name=\"b2\",  # ~b3-b4 size\n",
    "\ttrain_type=\"ptn\",\n",
    "\tdataset=\"vox2\",\n",
    "\tsource=\"github\",\n",
    ")\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(redim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redim.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(redim.parameters()).device\n",
    "print(f\"The model is loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_redim_embeddings(model, dataloader):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for batch in tqdm(dataloader, desc=\"Extracting ReDimNet Embeddings\"):\n",
    "        paths = batch['path']\n",
    "        waveforms = batch['waveform'].float().to(model.device)\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.forward(waveforms).cpu().numpy()\n",
    "        for path, embedding in zip(paths, batch_embeddings):\n",
    "            embeddings.append((path, embedding))\n",
    "    return embeddings\n",
    "\n",
    "# Example usage\n",
    "redim_embeddings = extract_redim_embeddings(redim, audio_dataloader)\n",
    "df['redim_embedding'] = [embedding for _, embedding in redim_embeddings]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def process_audio_and_extract_embeddings(model, df):\n",
    "    embeddings = []\n",
    "    model.eval()\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Audio\"):\n",
    "        audio_path = row['path']\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedding = model.forward(waveform).cpu().numpy()\n",
    "        \n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    df['embedding'] = embeddings\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df = process_audio_and_extract_embeddings(redim, df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('redim_subsample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 4 * 16_000\n",
    "audio_dataset = AudioDataset(df, max_len)\n",
    "audio_dataloader = audio_dataloader = DataLoader(\n",
    "    audio_dataset, batch_size=32, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_REDIMNET(model, dataloader):\n",
    "    all_embeddings = {}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating REDIMNET\"):\n",
    "            utts = batch[\"path\"]\n",
    "            features = batch[\"waveform\"].float().to('mps')\n",
    "            # Forward through model\n",
    "            embeds = model.forward(features).cpu().numpy()\n",
    "\n",
    "            for i, utt in enumerate(utts):\n",
    "                embed = embeds[i]\n",
    "                all_embeddings[utt] = embed\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "redim.to('mps')\n",
    "embeddings = evaluate_REDIMNET(redim, audio_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECAPA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# automatically checks for cached file, optionally set `cache_dir` location\n",
    "model_file = hf_hub_download(repo_id='Jenthe/ECAPA2', filename='ecapa2.pt', cache_dir=\"../models/ECAPA2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecapa2 = torch.jit.load(model_file, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(ecapa2.parameters()).device\n",
    "print(f\"The ECAPA2 model is loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecapa2.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"./wav_files_voxceleb2\"\n",
    "ecapa2_df = scan_directory_voxceleb2(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 5 * 16_000\n",
    "audio_dataset = AudioDataset(ecapa2_df, max_len)\n",
    "audio_dataloader = audio_dataloader = DataLoader(\n",
    "    audio_dataset, batch_size=32, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ECAPA2(model, dataloader):\n",
    "    all_embeddings = {}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating ECAPA2\"):\n",
    "            utts = batch[\"path\"]\n",
    "            features = batch[\"waveform\"].float().to(\"mps\")\n",
    "            # Forward through model\n",
    "            embeds = model.forward(features).cpu().numpy()\n",
    "\n",
    "            for i, utt in enumerate(utts):\n",
    "                embed = embeds[i]\n",
    "                all_embeddings[utt] = embed\n",
    "\n",
    "    return all_embeddings\n",
    "\n",
    "\n",
    "embeddings_ecapa2 = evaluate_ECAPA2(ecapa2, audio_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecapa2_df['embedding'] = ecapa2_df['path'].map(embeddings_ecapa2)\n",
    "\n",
    "# check if embeddings match\n",
    "ecapa2_df['embedding'][0] == embeddings_ecapa2[ecapa2_df['path'][0]]\n",
    "\n",
    "# save embeddings to disk\n",
    "csv_name = \"ecapa2_embeddings.csv\"\n",
    "ecapa2_df.to_csv(os.path.join(embeds_dir, csv_name), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class VariableLengthDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    paths = [item[\"path\"] for item in batch]\n",
    "    waveforms = [item[\"waveform\"] for item in batch]\n",
    "    return {\"path\": paths, \"waveform\": waveforms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "# Example data\n",
    "data = [\n",
    "    {'path': 'utt1', 'waveform': torch.randn(16000)},\n",
    "    {'path': 'utt2', 'waveform': torch.randn(32000)},\n",
    "    {'path': 'utt3', 'waveform': torch.randn(56000)},\n",
    "]\n",
    "\n",
    "dataset = VariableLengthDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_torch_model_various(model, dataloader, device):\n",
    "    all_embeddings = {}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            utts = batch[\"path\"]\n",
    "            waveforms = batch[\"waveform\"]\n",
    "\n",
    "            for utt, waveform in zip(utts, waveforms):\n",
    "                waveform = waveform.float().to(device).unsqueeze(0)# Add batch dimension\n",
    "                embed = model.forward(waveform).cpu().numpy().squeeze(0)\n",
    "# Remove batch dimension\n",
    "                all_embeddings[utt] = embed\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_embeddings = evaluate_torch_model_various(redimnet_model, dataloader, \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatasetVarious(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_path = self.dataframe.iloc[idx][\"path\"]\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        sample = {\"path\": audio_path, \"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    paths = [item[\"path\"] for item in batch]\n",
    "    waveforms = [item[\"waveform\"] for item in batch]\n",
    "    return {\"path\": paths, \"waveform\": waveforms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDatasetVarious(df)\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_embeddings = evaluate_torch_model_various(redimnet_model, dataloader, \"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windowed dataset to parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatasetFBank(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, max_len, model):\n",
    "        self.dataframe = dataframe\n",
    "        self.max_len = max_len\n",
    "        self.model = model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_path = self.dataframe.iloc[idx][\"path\"]\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "        # Extract fbank features\n",
    "        fbank = self.model.compute_fbank(waveform)\n",
    "\n",
    "        sample = {\"path\": audio_path, \"waveform\": fbank, \"sample_rate\": sample_rate}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "# Create an instance of the dataset\n",
    "# 1 s  = 16_000 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_CAMPPLUS(we_speaker_model, dataloader):\n",
    "    all_embeddings = {}\n",
    "    we_speaker_model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            utts = batch[\"path\"]\n",
    "            features = batch[\"waveform\"].float().to(we_speaker_model.device)\n",
    "            # Forward through model\n",
    "            outputs = we_speaker_model.model(features)  # embed or (embed_a, embed_b)\n",
    "            embeds = outputs[-1] if isinstance(outputs, tuple) else outputs\n",
    "            embeds = embeds.cpu().detach().numpy()\n",
    "\n",
    "            for i, utt in enumerate(utts):\n",
    "                embed = embeds[i]\n",
    "                all_embeddings[utt] = embed\n",
    "\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 347/347 [00:53<00:00,  6.49it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dir = \"data/voxceleb2_eval_segments\"\n",
    "df = scan_directory_voxceleb2(test_dir)\n",
    "\n",
    "\n",
    "campplus_model = wespeaker.load_model(\"campplus\")\n",
    "campplus_model.set_device(\"mps\")\n",
    "\n",
    "max_len = 4 * 16000\n",
    "audio_dataset_fbank = AudioDatasetFBank(df, max_len, campplus_model)\n",
    "\n",
    "# Create a DataLoader for the audio_dataset\n",
    "audio_dataloader_fbank = DataLoader(audio_dataset_fbank, batch_size=32, shuffle=False)\n",
    "\n",
    "embeddings = evaluate_CAMPPLUS(campplus_model, audio_dataloader_fbank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split file utteaerance_filename by \"_\" to get video_id, and frame_id which is a third element\n",
    "df['video_id'] = df['utterance_filename'].apply(lambda x: x.split(\"_\")[0])\n",
    "df['frame_id'] = df['utterance_filename'].apply(lambda x: x.split(\"_\")[2].replace(\".wav\", \"\"))\n",
    "df.drop(columns=['utterance_filename'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>person_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>frame_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/voxceleb2_eval_segments/id02019/87_seg_14...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>87</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/voxceleb2_eval_segments/id02019/115_seg_1...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/voxceleb2_eval_segments/id02019/96_seg_0.wav</td>\n",
       "      <td>id02019</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/voxceleb2_eval_segments/id02019/89_seg_4.wav</td>\n",
       "      <td>id02019</td>\n",
       "      <td>89</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/voxceleb2_eval_segments/id02019/1_seg_13.wav</td>\n",
       "      <td>id02019</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091</th>\n",
       "      <td>data/voxceleb2_eval_segments/id03347/313_seg_1...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>313</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11092</th>\n",
       "      <td>data/voxceleb2_eval_segments/id03347/34_seg_2.wav</td>\n",
       "      <td>id03347</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11093</th>\n",
       "      <td>data/voxceleb2_eval_segments/id03347/381_seg_3...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>381</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11094</th>\n",
       "      <td>data/voxceleb2_eval_segments/id03347/259_seg_2...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>259</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11095</th>\n",
       "      <td>data/voxceleb2_eval_segments/id03347/332_seg_3...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>332</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11096 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path person_id video_id  \\\n",
       "0      data/voxceleb2_eval_segments/id02019/87_seg_14...   id02019       87   \n",
       "1      data/voxceleb2_eval_segments/id02019/115_seg_1...   id02019      115   \n",
       "2      data/voxceleb2_eval_segments/id02019/96_seg_0.wav   id02019       96   \n",
       "3      data/voxceleb2_eval_segments/id02019/89_seg_4.wav   id02019       89   \n",
       "4      data/voxceleb2_eval_segments/id02019/1_seg_13.wav   id02019        1   \n",
       "...                                                  ...       ...      ...   \n",
       "11091  data/voxceleb2_eval_segments/id03347/313_seg_1...   id03347      313   \n",
       "11092  data/voxceleb2_eval_segments/id03347/34_seg_2.wav   id03347       34   \n",
       "11093  data/voxceleb2_eval_segments/id03347/381_seg_3...   id03347      381   \n",
       "11094  data/voxceleb2_eval_segments/id03347/259_seg_2...   id03347      259   \n",
       "11095  data/voxceleb2_eval_segments/id03347/332_seg_3...   id03347      332   \n",
       "\n",
       "      frame_id  \n",
       "0           14  \n",
       "1            1  \n",
       "2            0  \n",
       "3            4  \n",
       "4           13  \n",
       "...        ...  \n",
       "11091       16  \n",
       "11092        2  \n",
       "11093        3  \n",
       "11094       22  \n",
       "11095        3  \n",
       "\n",
       "[11096 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.utils import map_embeddings_to_df\n",
    "\n",
    "df = map_embeddings_to_df(df, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['path'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>frame_id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02019</td>\n",
       "      <td>87</td>\n",
       "      <td>14</td>\n",
       "      <td>[-0.7572477, -0.28664908, 1.2824339, -0.092000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id02019</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.8128398, 1.931974, 1.5611752, 0.6306397, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id02019</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.3887814, 0.728199, 0.86692244, -0.42631707,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id02019</td>\n",
       "      <td>89</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.75237453, -0.14510478, 0.0634555, -0.42303...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id02019</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>[-1.2547182, -0.08362903, 0.39995107, -0.02934...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091</th>\n",
       "      <td>id03347</td>\n",
       "      <td>313</td>\n",
       "      <td>16</td>\n",
       "      <td>[-0.40491396, 0.82559365, -0.06662895, -0.9083...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11092</th>\n",
       "      <td>id03347</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.5628453, -1.0667058, -0.295191, 1.0603111,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11093</th>\n",
       "      <td>id03347</td>\n",
       "      <td>381</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.12918745, -0.61040545, -0.3202267, -0.2740...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11094</th>\n",
       "      <td>id03347</td>\n",
       "      <td>259</td>\n",
       "      <td>22</td>\n",
       "      <td>[-0.38162476, 0.37319005, 0.9816289, -0.403025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11095</th>\n",
       "      <td>id03347</td>\n",
       "      <td>332</td>\n",
       "      <td>3</td>\n",
       "      <td>[-1.319654, -0.62635785, 0.12563337, -0.104788...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11096 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      person_id video_id frame_id  \\\n",
       "0       id02019       87       14   \n",
       "1       id02019      115        1   \n",
       "2       id02019       96        0   \n",
       "3       id02019       89        4   \n",
       "4       id02019        1       13   \n",
       "...         ...      ...      ...   \n",
       "11091   id03347      313       16   \n",
       "11092   id03347       34        2   \n",
       "11093   id03347      381        3   \n",
       "11094   id03347      259       22   \n",
       "11095   id03347      332        3   \n",
       "\n",
       "                                               embedding  \n",
       "0      [-0.7572477, -0.28664908, 1.2824339, -0.092000...  \n",
       "1      [0.8128398, 1.931974, 1.5611752, 0.6306397, -0...  \n",
       "2      [0.3887814, 0.728199, 0.86692244, -0.42631707,...  \n",
       "3      [-0.75237453, -0.14510478, 0.0634555, -0.42303...  \n",
       "4      [-1.2547182, -0.08362903, 0.39995107, -0.02934...  \n",
       "...                                                  ...  \n",
       "11091  [-0.40491396, 0.82559365, -0.06662895, -0.9083...  \n",
       "11092  [-0.5628453, -1.0667058, -0.295191, 1.0603111,...  \n",
       "11093  [-0.12918745, -0.61040545, -0.3202267, -0.2740...  \n",
       "11094  [-0.38162476, 0.37319005, 0.9816289, -0.403025...  \n",
       "11095  [-1.319654, -0.62635785, 0.12563337, -0.104788...  \n",
       "\n",
       "[11096 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_embeddings_to_parquet\n",
    "\n",
    "os.makedirs(\"embeds/voxceleb2_segments\", exist_ok=True)\n",
    "save_embeddings_to_parquet(df, \"embeds/voxceleb2_segments/campplus_embeds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import read_embeddings_from_parquet\n",
    "\n",
    "\n",
    "audio_df = read_embeddings_from_parquet(\"embeds/voxceleb2_segments/campplus_embeds\")\n",
    "video_df = read_embeddings_from_parquet(\"embeds/facenet_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8941 entries, 0 to 8940\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   video_id   8941 non-null   object \n",
      " 1   person_id  8941 non-null   object \n",
      " 2   embedding  8449 non-null   object \n",
      " 3   frame_id   8449 non-null   float64\n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 279.5+ KB\n"
     ]
    }
   ],
   "source": [
    "video_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>person_id</th>\n",
       "      <th>utterance_filename</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id02019/50_seg...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>50_seg3_.wav</td>\n",
       "      <td>[0.4952665, 0.22981328, 0.81912225, 0.6312957,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id02019/194_se...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>194_seg0_.wav</td>\n",
       "      <td>[0.061572704, 0.14836346, 1.0915542, -0.358542...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id02019/196_se...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>196_seg8_.wav</td>\n",
       "      <td>[0.14852108, -0.22755864, 0.08376154, 0.225873...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id02019/87_seg...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>87_seg8_.wav</td>\n",
       "      <td>[-0.010013767, 0.37227827, 0.76968944, -0.4603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id02019/1_seg9...</td>\n",
       "      <td>id02019</td>\n",
       "      <td>1_seg9_.wav</td>\n",
       "      <td>[-1.1399962, 1.3456193, -0.090061635, 0.067406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id03347/268_se...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>268_seg8_.wav</td>\n",
       "      <td>[0.106771335, -0.60677135, 0.6916544, 0.571006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11092</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id03347/323_se...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>323_seg4_.wav</td>\n",
       "      <td>[-1.2123433, 1.0951328, 0.07456491, -0.0196330...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11093</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id03347/313_se...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>313_seg17_.wav</td>\n",
       "      <td>[-0.23038934, 0.4561194, 0.17479698, -0.952375...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11094</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id03347/259_se...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>259_seg15_.wav</td>\n",
       "      <td>[0.0460768, -0.34532627, 0.29578942, -0.645461...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11095</th>\n",
       "      <td>../data/voxceleb2_eval_segments/id03347/160_se...</td>\n",
       "      <td>id03347</td>\n",
       "      <td>160_seg0_.wav</td>\n",
       "      <td>[-0.27928707, -0.13588534, 0.0614033, 0.004813...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11096 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path person_id  \\\n",
       "0      ../data/voxceleb2_eval_segments/id02019/50_seg...   id02019   \n",
       "1      ../data/voxceleb2_eval_segments/id02019/194_se...   id02019   \n",
       "2      ../data/voxceleb2_eval_segments/id02019/196_se...   id02019   \n",
       "3      ../data/voxceleb2_eval_segments/id02019/87_seg...   id02019   \n",
       "4      ../data/voxceleb2_eval_segments/id02019/1_seg9...   id02019   \n",
       "...                                                  ...       ...   \n",
       "11091  ../data/voxceleb2_eval_segments/id03347/268_se...   id03347   \n",
       "11092  ../data/voxceleb2_eval_segments/id03347/323_se...   id03347   \n",
       "11093  ../data/voxceleb2_eval_segments/id03347/313_se...   id03347   \n",
       "11094  ../data/voxceleb2_eval_segments/id03347/259_se...   id03347   \n",
       "11095  ../data/voxceleb2_eval_segments/id03347/160_se...   id03347   \n",
       "\n",
       "      utterance_filename                                          embedding  \n",
       "0           50_seg3_.wav  [0.4952665, 0.22981328, 0.81912225, 0.6312957,...  \n",
       "1          194_seg0_.wav  [0.061572704, 0.14836346, 1.0915542, -0.358542...  \n",
       "2          196_seg8_.wav  [0.14852108, -0.22755864, 0.08376154, 0.225873...  \n",
       "3           87_seg8_.wav  [-0.010013767, 0.37227827, 0.76968944, -0.4603...  \n",
       "4            1_seg9_.wav  [-1.1399962, 1.3456193, -0.090061635, 0.067406...  \n",
       "...                  ...                                                ...  \n",
       "11091      268_seg8_.wav  [0.106771335, -0.60677135, 0.6916544, 0.571006...  \n",
       "11092      323_seg4_.wav  [-1.2123433, 1.0951328, 0.07456491, -0.0196330...  \n",
       "11093     313_seg17_.wav  [-0.23038934, 0.4561194, 0.17479698, -0.952375...  \n",
       "11094     259_seg15_.wav  [0.0460768, -0.34532627, 0.29578942, -0.645461...  \n",
       "11095      160_seg0_.wav  [-0.27928707, -0.13588534, 0.0614033, 0.004813...  \n",
       "\n",
       "[11096 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "audio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
