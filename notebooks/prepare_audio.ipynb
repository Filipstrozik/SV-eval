{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "\n",
    "def process_wav_to_stack(audio_waveform: torch.Tensor, length: int = 4, step: int=1, sample_rate: int=16_000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This function takes a waveform tensor and returns a stack of audio tensors of length `length` seconds, with a step of `step` seconds.\n",
    "\n",
    "    Args:\n",
    "    - audio_waveform: torch.Tensor of shape (1,num_samples)\n",
    "    \"\"\"\n",
    "\n",
    "    # check if input tensor is 2D (1, num_samples)\n",
    "    if len(audio_waveform.shape) != 2:\n",
    "        raise ValueError(f\"Expected a 2D tensor, got a {len(audio_waveform.shape)}D tensor\")\n",
    "\n",
    "    num_samples = audio_waveform.shape[1] \n",
    "    segment_length = length * sample_rate\n",
    "    step_length = step * sample_rate\n",
    "\n",
    "    if num_samples < segment_length:\n",
    "        raise ValueError(\"The audio waveform is too short to create even one segment\")\n",
    "\n",
    "    audio_segments = [\n",
    "        audio_waveform[:, i * step_length : i * step_length + segment_length]\n",
    "        for i in range((num_samples - segment_length) // step_length + 1)\n",
    "    ]\n",
    "    # return stack of tensors should be of shape (num_segments, 1, segment_length)\n",
    "    audio_tensor = torch.stack(audio_segments)\n",
    "    return audio_tensor\n",
    "\n",
    "def get_audio_tensor(audio: Union[str, Path, torch.Tensor], length: int = 4, step: int=1, sample_rate: int=16_000) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This function takes a path to a wav file or a torch tensor and returns a stack of audio tensors of length `length` seconds, with a step of `step` seconds.\n",
    "\n",
    "    Args:\n",
    "    - audio: Union[str, Path, torch.Tensor]\n",
    "    - length: int\n",
    "    - step: int\n",
    "    - sample_rate: int\n",
    "    \"\"\"\n",
    "    if isinstance(audio, (str, Path)):\n",
    "        audio_waveform, sample_rate = torchaudio.load(audio)\n",
    "    elif isinstance(audio, torch.Tensor):\n",
    "        audio_waveform = audio\n",
    "    else:\n",
    "        raise ValueError(\"Expected a path to a wav file or a torch tensor\")\n",
    "\n",
    "    audio_tensor = process_wav_to_stack(audio_waveform, length, step, sample_rate)\n",
    "    return audio_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a audio using torchaudio by path and get a stack of audio tensors\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "\n",
    "path = Path(\"../data/voxceleb2_wav_eval/id00817/21.wav\")\n",
    "audio_waveform, sample_rate = torchaudio.load(path)\n",
    "display(Audio(audio_waveform, rate=sample_rate))\n",
    "print(audio_waveform.shape)\n",
    "audio_tensor = get_audio_tensor(audio_waveform, length=4, step=1)\n",
    "print(audio_tensor.shape)\n",
    "\n",
    "#iterate through the first dimension of the audio tensor\n",
    "for audio in audio_tensor:\n",
    "    display(Audio(audio, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
