{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import os\n",
    "import wespeaker\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import subprocess\n",
    "import sys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_directory_voxceleb1(test_dir):\n",
    "    data = []\n",
    "    for person_id in os.listdir(test_dir):\n",
    "        person_path = os.path.join(test_dir, person_id)\n",
    "        if os.path.isdir(person_path):\n",
    "            for utterance_env in os.listdir(person_path):\n",
    "                utterance_path = os.path.join(person_path, utterance_env)\n",
    "                if os.path.isdir(utterance_path):\n",
    "                    for file in os.listdir(utterance_path):\n",
    "                        file_path = os.path.join(utterance_path, file)\n",
    "                        if os.path.isfile(file_path):\n",
    "                            # Assuming embedding is a placeholder for actual embedding extraction\n",
    "                            embedding = \"embedding_placeholder\"\n",
    "                            waveform, sample_rate = torchaudio.load(file_path)\n",
    "                            duration = waveform.shape[1] / sample_rate\n",
    "                            data.append(\n",
    "                                [file_path, person_id, utterance_env, file, embedding, duration]\n",
    "                            )\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\n",
    "            \"path\",\n",
    "            \"person_id\",\n",
    "            \"utterance_env\",\n",
    "            \"utterance_filename\",\n",
    "            \"embedding\",\n",
    "            \"duration\",\n",
    "        ],\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "test_dir = \"../data/vox1_test_wav\"\n",
    "df = scan_directory_voxceleb1(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_directory_voxceleb2(test_dir):\n",
    "    data = []\n",
    "    for person_id in os.listdir(test_dir):\n",
    "        person_path = os.path.join(test_dir, person_id)\n",
    "        if os.path.isdir(person_path):\n",
    "            for file in os.listdir(person_path):\n",
    "                file_path = os.path.join(person_path, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Assuming embedding is a placeholder for actual embedding extraction\n",
    "                    embedding = \"embedding_placeholder\"\n",
    "                    waveform, sample_rate = torchaudio.load(file_path)\n",
    "                    duration = waveform.shape[1] / sample_rate\n",
    "                    data.append([file_path, person_id, file, embedding, duration])\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data, columns=[\"path\", \"person_id\", \"utterance_filename\", \"embedding\", \"duration\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "test_dir = \"../data/vox1_test_wav_windowed\"\n",
    "df = scan_directory_voxceleb2(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load waveforms and calculate durations\n",
    "\n",
    "\n",
    "# Load waveforms and calculate durations\n",
    "\n",
    "\n",
    "# Plot the durations\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['duration'], bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Audio Clip Durations')\n",
    "plt.xlabel('Duration (seconds)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 8\n",
    "total_files = 0\n",
    "for duration in df['duration']:\n",
    "    num_windows = int(duration // window_size)\n",
    "    if duration % window_size > 0:\n",
    "        num_windows += 1\n",
    "    total_files += num_windows\n",
    "\n",
    "total_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import full\n",
    "\n",
    "\n",
    "def repeat_to_max_len(data, max_len):\n",
    "    \"\"\"Repeat to a single wave to the specified length.\n",
    "\n",
    "    Args:\n",
    "        data: torch.Tensor (random len)\n",
    "        max_len: maximum length to repeat or cut the data\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor (repeated to max_len)\n",
    "    \"\"\"\n",
    "    data_len = data.shape[1]\n",
    "\n",
    "    if data_len == 0:\n",
    "        raise ValueError(\"data_len should not be zero\")\n",
    "\n",
    "    if data_len < max_len:\n",
    "        repeats = max_len // data_len\n",
    "        remainder = max_len % data_len\n",
    "        data = torch.cat([data] * repeats, dim=1)\n",
    "        if remainder > 0:\n",
    "            data = torch.cat([data, data[:, :remainder]], dim=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def allign_dataframe_durations_celeb2(df, window_size, new_dataset_dir):\n",
    "    if not os.path.exists(new_dataset_dir):\n",
    "        os.makedirs(new_dataset_dir)\n",
    "\n",
    "    new_data = []\n",
    "    for index, row in df.iterrows():\n",
    "        person_dir = os.path.join(new_dataset_dir, row[\"person_id\"])\n",
    "        if not os.path.exists(person_dir):\n",
    "            os.makedirs(person_dir)\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(row[\"path\"])\n",
    "        max_len = window_size * sample_rate  # 8 * 16000 = 128000\n",
    "\n",
    "        # if it is less than 8 seconds - repeat to 8 seconds\n",
    "        if waveform.shape[1] < max_len:\n",
    "            waveform = repeat_to_max_len(waveform, max_len)\n",
    "            file_name = row['utterance_filename']\n",
    "            full_path = os.path.join(person_dir, file_name) \n",
    "            torchaudio.save(full_path, waveform, sample_rate)\n",
    "            new_data.append(\n",
    "                [\n",
    "                    os.path.join(person_dir, f\"{row['utterance_filename']}.wav\"),\n",
    "                    row[\"person_id\"],\n",
    "                    f\"{row['utterance_filename']}.wav\"\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            # if it is more, lets devide it into 8 second windows, and last one will be less than 8 seconds so repeat it to 8 seconds\n",
    "            num_windows = waveform.shape[1] // max_len\n",
    "            remainder = waveform.shape[1] % max_len\n",
    "\n",
    "            for i in range(num_windows):\n",
    "                start = i * max_len\n",
    "                end = (i + 1) * max_len\n",
    "                window = waveform[:, start:end]\n",
    "                file_name = row[\"utterance_filename\"].replace(\".wav\", f\"_{i}.wav\")\n",
    "                full_path = os.path.join(person_dir, file_name)\n",
    "                torchaudio.save(full_path, window, sample_rate)\n",
    "                new_data.append(\n",
    "                    [\n",
    "                        full_path,  \n",
    "                        row[\"person_id\"],\n",
    "                        file_name\n",
    "                    ]\n",
    "                )\n",
    "            if remainder > 0:\n",
    "                start = max_len * num_windows\n",
    "                end = waveform.shape[1]\n",
    "\n",
    "                window = waveform[:, start:end]\n",
    "                window = repeat_to_max_len(window, max_len)\n",
    "                file_name = row[\"utterance_filename\"].replace(\".wav\", f\"_{num_windows}.wav\")\n",
    "                full_path = os.path.join(person_dir, file_name)\n",
    "                torchaudio.save(full_path, window, sample_rate)\n",
    "                new_data.append(\n",
    "                    [\n",
    "                        full_path,\n",
    "                        row[\"person_id\"],\n",
    "                        file_name\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "    new_df = pd.DataFrame(\n",
    "        new_data,\n",
    "        columns=[\"path\", \"person_id\", \"utterance_filename\"],\n",
    "    )\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "window_size = 8\n",
    "new_dataset_dir = \"../data/vox1_test_wav_windowed\"\n",
    "new_df = allign_dataframe_durations_celeb2(df, window_size, new_dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO if you do not specify environment dirs it will override files...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.info()\n",
    "unique_paths = new_df['path'].unique()\n",
    "len(unique_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
